# Transcribe and Translate Subtitles

## ğŸš¨ Important Note
- **This project is for non-commercial use only!**
- **Every task runs locally without internet, ensuring maximum privacy.**

---

## âœ¨ Features  
This project is built on ONNX Runtime and the IPEX-LLM framework.
- VAD Support:
  - [Faster_Whisper - Silero](https://github.com/SYSTRAN/faster-whisper/blob/master/faster_whisper/vad.py)
  - [Official - Silero](https://github.com/snakers4/silero-vad)
  - [FSMN](https://modelscope.cn/models/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch)

- Deoiser Support:
  - [ZipEnhancer](https://modelscope.cn/models/iic/speech_zipenhancer_ans_multiloss_16k_base)
  - [GTCRN](https://github.com/Xiaobin-Rong/gtcrn)
  - [DFSMN](https://modelscope.cn/models/iic/speech_dfsmn_ans_psm_48k_causal)

- ASR Support:
  - [SenseVoiceSmall](https://modelscope.cn/models/iic/SenseVoiceSmall)
  - [Paraformer](https://modelscope.cn/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch)
  - [Whisper-Large-V3](https://huggingface.co/openai/whisper-large-v3)
  - [Whisper-Large-V3-Turbo](https://huggingface.co/openai/whisper-large-v3-turbo)
  - Custom-Whisper - V2 / V3 / Distil / Turbo (The model must be exported using this [tool](https://github.com/DakeQQ/Automatic-Speech-Recognition-ASR-ONNX))

- LLM Supports: 
  - Gemma-2-it: [2B](https://huggingface.co/google/gemma-2-2b-it), [9B](https://huggingface.co/google/gemma-2-9b-it)  
  - GLM4-Chat: [9B](https://github.com/THUDM/GLM-4)  
  - MiniCPM3: [4B](https://github.com/OpenBMB/MiniCPM)  
  - Phi3/3.5: [mini](https://huggingface.co/microsoft/Phi-3.5-mini-instruct), [medium](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct)  
  - Qwen2.5-Instruct: [3B](https://modelscope.cn/models/Qwen/Qwen2.5-3B-Instruct), [7B](https://modelscope.cn/models/Qwen/Qwen2.5-7B-Instruct), [14B](https://modelscope.cn/models/Qwen/Qwen2.5-14B-Instruct), [32B](https://www.modelscope.cn/models/Qwen/Qwen2.5-32B-Instruct)  
  - GGUF formatï¼šiq1_s, iq2_xs, iq2_xxs, q4k_s, q4k_m


---

## ğŸ“‹ Setup Instructions

### âœ… Step 1: Install Dependencies
- Run the following command in your terminal to install the latest required Python packages:
- For Apple Silicon M-series chips, avoid installing `onnxruntime-openvino`, as it will cause errors.
```bash
conda install ffmpeg

pip install -r requirements.txt
```

### ğŸ“¥ Step 2: Download Necessary Models
- Download the required models from Google Drive: [Transcribe_and_Translate_Subtitles](https://drive.google.com/drive/folders/1W5yqPm-FYD2r1KR7JrDwJ8jzuFALNr9O?usp=drive_link). After downloading, unzip the file.
- Or [Baidu Cloud](https://pan.baidu.com/s/1DSAYmbMX5lKj9oz8Uhmmwg?pwd=dake) (This link lacks a zip package; download files individually as Baidu Cloud doesn't support packages over 4GB.)


### ğŸ¤– Step 3: Download a Preferred LLM Model (Optional, for translate task)
- Choose and download your preferred LLM model.
- The largest LLM size that can run on a 16GB RAM computer is 7 billion parameters (7B). For example: [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)

### ğŸ“‚ Step 4: Place the LLM Model in the Correct Directory (Optional, for translate task)
- Move the downloaded LLM model to the following path:
```
Transcribe_and_Translate_Subtitles/LLM/Qwen/7B
```

### ğŸ–¥ï¸ Step 5: Download and Place `run.py`
- Download the `run.py` script from this repository.
- Place it in the `Transcribe_and_Translate_Subtitles` folder.

### ğŸ“ Step 6: Place Target Videos in the Media Folder
- Place the videos you want to transcribe and translate in the following directory:
```
Transcribe_and_Translate_Subtitles/Media
```
- The application will process the videos one by one.

### ğŸš€ Step 7: Run the Application
- Open your preferred terminal (PyCharm, CMD, PowerShell, etc.).
- Execute the following command to start the application:
```bash
python run.py
```
- Once the application starts, you will see a webpage open in your browser.
   ![screenshot](https://github.com/DakeQQ/Transcribe-and-Translate-Subtitles/blob/main/screen/Screenshot%20from%202025-01-13%2000-47-34.png)

### ğŸ› ï¸ Step 8: Fix Silero-VAD Error (if encountered)
- On the first run, you might encounter a **Silero-VAD error**. Simply restart the application, and it should be resolved.

### ğŸ’» Step 9: Device Support
- This project currently supports: **Intel-CPU-GPU-NPU**, **AMD-CPU** and **Apple-CPU** users.

### ğŸ”§ Step 10: Enable Intel-GPU or Intel-NPU (Optional)
- The LLM integration is based on the [ipex-llm](https://github.com/intel-analytics/ipex-llm). To use Intel-GPU or Intel-NPU, follow the instructions in the `ipex-llm` repository to enable these devices (now only support python3.11). Without this setup, the application will not work on GPU/NPU hardware.

---

## ğŸ‰ Enjoy the Application!
```
Transcribe_and_Translate_Subtitles/Results/Subtitles
```
---

## ğŸ“Œ To-Do List
- [ ] Add support for NVIDIA devices
- [ ] Add support for AMD devices
- [ ] Real-Time Translate & Trascribe Video Player

---
### æ€§èƒ½ Performance  
| OS           | Backend           | Denoiser          | VAD                  | ASR                  | Real-Time Factor<br>test_video.mp4<br>7602 seconds |
|:------------:|:-----------------:|:-----------------:|:--------------------:|:--------------------:|:----------------:|
| Ubuntu-24.04 | CPU <br> i3-12300 | None              | Silero               | SenseVoiceSmall      |        0.08      |
| Ubuntu-24.04 | CPU <br> i3-12300 | GTCRN             | Silero               | SenseVoiceSmall      |        0.10      |
| Ubuntu-24.04 | CPU <br> i3-12300 | GTCRN             | FSMN                 | SenseVoiceSmall      |        0.054     |
| Ubuntu-24.04 | CPU <br> i3-12300 | ZipEnhancer       | FSMN                 | SenseVoiceSmall      |        0.39      |
| Ubuntu-24.04 | CPU <br> i3-12300 | GTCRN             | Silero               | Whisper-Large-V3     |        0.20      |
| Ubuntu-24.04 | CPU <br> i3-12300 | GTCRN             | FSMN                 | Whisper-Large-V3-Turbo |        0.148   |

---
# è½¬å½•å’Œç¿»è¯‘å­—å¹•

## ğŸš¨ é‡è¦æç¤º  
- **æ­¤é¡¹ç›®ä»…é™éå•†ä¸šç”¨é€”ï¼**  
- **æ‰€æœ‰ä»»åŠ¡å‡åœ¨æœ¬åœ°è¿è¡Œï¼Œæ— éœ€è¿æ¥äº’è”ç½‘ï¼Œç¡®ä¿æœ€å¤§ç¨‹åº¦çš„éšç§ä¿æŠ¤ã€‚**

---

## âœ¨ åŠŸèƒ½
è¿™ä¸ªé¡¹ç›®åŸºäºONNX Runtimeå’ŒIPEX-LLMæ¡†æ¶ã€‚
- **è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆVADï¼‰æ”¯æŒ**ï¼š
  - [Faster_Whisper - Silero](https://github.com/SYSTRAN/faster-whisper/blob/master/faster_whisper/vad.py)
  - [å®˜æ–¹ - Silero](https://github.com/snakers4/silero-vad)
  - [FSMN](https://modelscope.cn/models/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch)

- **å»å™ªå™¨ (Denoiser) æ”¯æŒ**ï¼š
  - [ZipEnhancer](https://modelscope.cn/models/iic/speech_zipenhancer_ans_multiloss_16k_base)
  - [GTCRN](https://github.com/Xiaobin-Rong/gtcrn)
  - [DFSMN](https://modelscope.cn/models/iic/speech_dfsmn_ans_psm_48k_causal)

- **è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ”¯æŒ**ï¼š
  - [SenseVoiceSmall](https://modelscope.cn/models/iic/SenseVoiceSmall)
  - [Paraformer](https://modelscope.cn/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch)
  - [Whisper-Large-V3](https://huggingface.co/openai/whisper-large-v3)
  - [Whisper-Large-V3-Turbo](https://huggingface.co/openai/whisper-large-v3-turbo)
  - è‡ªå®šä¹‰ Whisper - V2 / V3 / Distil / Turboï¼ˆéœ€è¦ä½¿ç”¨æ­¤[å·¥å…·](https://github.com/DakeQQ/Automatic-Speech-Recognition-ASR-ONNX)å¯¼å‡ºæ¨¡å‹ï¼‰

- **å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ”¯æŒ**ï¼š  
  - Gemma-2-it: [2B](https://huggingface.co/google/gemma-2-2b-it), [9B](https://huggingface.co/google/gemma-2-9b-it)  
  - GLM4-Chat: [9B](https://github.com/THUDM/GLM-4)  
  - MiniCPM3: [4B](https://github.com/OpenBMB/MiniCPM)  
  - Phi3/3.5: [mini](https://huggingface.co/microsoft/Phi-3.5-mini-instruct), [medium](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct)  
  - Qwen2.5-Instruct: [3B](https://modelscope.cn/models/Qwen/Qwen2.5-3B-Instruct), [7B](https://modelscope.cn/models/Qwen/Qwen2.5-7B-Instruct), [14B](https://modelscope.cn/models/Qwen/Qwen2.5-14B-Instruct), [32B](https://www.modelscope.cn/models/Qwen/Qwen2.5-32B-Instruct)  
  - GGUF æ ¼å¼ï¼šiq1_s, iq2_xs, iq2_xxs, q4k_s, q4k_m  

---

## ğŸ“‹ è®¾ç½®æŒ‡å—

### âœ… ç¬¬ä¸€æ­¥ï¼šå®‰è£…ä¾èµ–é¡¹  
- åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…æ‰€éœ€çš„æœ€æ–° Python åŒ…ï¼š
- å¯¹äºè‹¹æœ M ç³»åˆ—èŠ¯ç‰‡ï¼Œè¯·ä¸è¦å®‰è£… `onnxruntime-openvino`ï¼Œå¦åˆ™ä¼šå¯¼è‡´é”™è¯¯ã€‚
```bash
conda install ffmpeg

pip install -r requirements.txt
```

### ğŸ“¥ ç¬¬äºŒæ­¥ï¼šä¸‹è½½å¿…è¦çš„æ¨¡å‹  
- ä» Google Drive ä¸‹è½½æ‰€éœ€æ¨¡å‹ï¼š[Transcribe_and_Translate_Subtitles](https://drive.google.com/drive/folders/1W5yqPm-FYD2r1KR7JrDwJ8jzuFALNr9O?usp=drive_link), ä¸‹è½½å®Œæˆåï¼Œè§£å‹æ–‡ä»¶ã€‚
- æˆ– [ç™¾åº¦äº‘](https://pan.baidu.com/s/1DSAYmbMX5lKj9oz8Uhmmwg?pwd=dake) (è¯¥é“¾æ¥æ²¡æœ‰æä¾›å‹ç¼©åŒ…ï¼›éœ€è¦é€ä¸ªä¸‹è½½æ–‡ä»¶å¤¹ï¼Œå› ä¸ºç™¾åº¦äº‘ä¸æ”¯æŒè¶…è¿‡ 4GB çš„å‹ç¼©åŒ…ã€‚)

### ğŸ¤– ç¬¬ä¸‰æ­¥ï¼šä¸‹è½½ä½ å–œæ¬¢çš„ LLM æ¨¡å‹ ï¼ˆå¯é€‰ï¼Œç”¨äºç¿»è¯‘ä»»åŠ¡ï¼‰ 
- é€‰æ‹©å¹¶ä¸‹è½½ä½ åå¥½çš„ LLM æ¨¡å‹ã€‚
- åœ¨16GBå†…å­˜çš„ç”µè„‘ä¸Šå¯è¿è¡Œçš„æœ€å¤§LLMæ¨¡å‹ä¸º70äº¿å‚æ•°(7B)ã€‚ä¾‹å¦‚ï¼š[Qwen2.5-7B-Instruct](https://modelscope.cn/models/Qwen/Qwen2.5-7B-Instruct)

### ğŸ“‚ ç¬¬å››æ­¥ï¼šå°† LLM æ¨¡å‹æ”¾ç½®åˆ°æ­£ç¡®çš„ç›®å½• ï¼ˆå¯é€‰ï¼Œç”¨äºç¿»è¯‘ä»»åŠ¡ï¼‰ 
- å°†ä¸‹è½½çš„ LLM æ¨¡å‹ç§»åŠ¨åˆ°ä»¥ä¸‹è·¯å¾„ï¼š  
```
Transcribe_and_Translate_Subtitles/LLM/Qwen/7B
```

### ğŸ–¥ï¸ ç¬¬äº”æ­¥ï¼šä¸‹è½½å¹¶æ”¾ç½® `run.py`  
- ä»æ­¤é¡¹ç›®çš„ä»“åº“ä¸‹è½½ `run.py` è„šæœ¬ã€‚  
- å°† `run.py` æ”¾ç½®åœ¨ `Transcribe_and_Translate_Subtitles` æ–‡ä»¶å¤¹ä¸­ã€‚

### ğŸ“ ç¬¬å…­æ­¥ï¼šå°†ç›®æ ‡è§†é¢‘æ”¾å…¥ Media æ–‡ä»¶å¤¹  
- å°†ä½ æƒ³è¦è½¬å½•å’Œç¿»è¯‘çš„è§†é¢‘æ”¾ç½®åœ¨ä»¥ä¸‹ç›®å½•ï¼š  
```
Transcribe_and_Translate_Subtitles/Media
```
- åº”ç”¨ç¨‹åºå°†é€ä¸ªå¤„ç†è¿™äº›è§†é¢‘ã€‚

### ğŸš€ ç¬¬ä¸ƒæ­¥ï¼šè¿è¡Œåº”ç”¨ç¨‹åº  
- æ‰“å¼€ä½ å–œæ¬¢çš„ç»ˆç«¯å·¥å…·ï¼ˆPyCharmã€CMDã€PowerShell ç­‰ï¼‰ã€‚  
- è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å¯åŠ¨åº”ç”¨ç¨‹åºï¼š  
```bash
python run.py
```
- åº”ç”¨ç¨‹åºå¯åŠ¨åï¼Œä½ çš„æµè§ˆå™¨å°†è‡ªåŠ¨æ‰“å¼€ä¸€ä¸ªç½‘é¡µã€‚  
   ![screenshot](https://github.com/DakeQQ/Transcribe-and-Translate-Subtitles/blob/main/screen/Screenshot%20from%202025-01-13%2000-53-21.png)

### ğŸ› ï¸ ç¬¬å…«æ­¥ï¼šä¿®å¤ Silero-VAD é”™è¯¯ï¼ˆå¦‚æœ‰ï¼‰  
- é¦–æ¬¡è¿è¡Œæ—¶ï¼Œä½ å¯èƒ½ä¼šé‡åˆ° **Silero-VAD é”™è¯¯**ã€‚åªéœ€é‡å¯åº”ç”¨ç¨‹åºå³å¯è§£å†³è¯¥é—®é¢˜ã€‚

### ğŸ’» ç¬¬ä¹æ­¥ï¼šæ”¯æŒè®¾å¤‡  
- æ­¤é¡¹ç›®ç›®å‰æ”¯æŒ **Intel-CPU-GPU-NPU**, **AMD-CPU** å’Œ **Apple-CPU** ç”¨æˆ·ã€‚

### ğŸ”§ ç¬¬åæ­¥ï¼šå¯ç”¨ Intel-GPU æˆ– Intel-NPUï¼ˆå¯é€‰ï¼‰  
- æ­¤é¡¹ç›®çš„ LLM é›†æˆåŸºäº [ipex-llm](https://github.com/intel-analytics/ipex-llm)ã€‚è‹¥è¦ä½¿ç”¨ Intel-GPU æˆ– Intel-NPUï¼Œè¯·æŒ‰ç…§ `ipex-llm` ä»“åº“ä¸­çš„è¯´æ˜æ¥å¯ç”¨è¿™äº›è®¾å¤‡(ç°åœ¨åªæ”¯æŒpython3.11)ã€‚å¦‚æœä¸è¿›è¡Œæ­¤è®¾ç½®ï¼Œåº”ç”¨ç¨‹åºå°†æ— æ³•åœ¨ GPU/NPU ç¡¬ä»¶ä¸Šè¿è¡Œã€‚

---

## ğŸ‰ å°½æƒ…äº«å—åº”ç”¨ç¨‹åºå§ï¼
```
Transcribe_and_Translate_Subtitles/Results/Subtitles
```
---

## ğŸ“Œ å¾…åŠäº‹é¡¹  
- [ ] æ·»åŠ å¯¹ NVIDIA è®¾å¤‡çš„æ”¯æŒ  
- [ ] æ·»åŠ å¯¹ AMD è®¾å¤‡çš„æ”¯æŒ  
- [ ] å®ç°å®æ—¶è§†é¢‘è½¬å½•å’Œç¿»è¯‘æ’­æ”¾å™¨
---
