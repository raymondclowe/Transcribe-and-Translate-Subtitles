# Transcribe and Translate Subtitles

## ğŸš¨ Important Note
- **Every task runs locally without internet, ensuring maximum privacy.**

---
## Updates
- 2025/7/5
    - Added a noise reduction model: MossFormerGAN_SE_16K
- 2025/6/11
    - Added HumAware-VAD, NVIDIA-NeMo-VAD, TEN-VAD
- 2025/6/3
    - Added Dolphin ASR model to support Asian languages.
- 2025/5/13
    - Added Float16/32 ASR models to support CUDA/DirectML GPU usage. These models can achieve >99% GPU operator deployment.
- 2025/5/9
    - Added an option to **not use** VAD (Voice Activity Detection), offering greater flexibility.
    - Added a noise reduction model: **MelBandRoformer**.
    - Added three Japanese anime fine-tuned Whisper models.
    - Added ASR model: **CrisperWhisper**.
    - Added English fine-tuned ASR model: **Whisper-Large-v3.5-Distil**.
    - Added ASR model supporting Chinese (including some dialects): **FireRedASR-AED-L**.
    - Removed the IPEX-LLM framework to enhance overall performance.
    - Cancelled LLM quantization options, standardizing on the **Q4F32** format.
    - Improved accuracy of **FSMN-VAD**.
    - Improved recognition accuracy of **Paraformer**.
    - Improved recognition accuracy of **SenseVoice**.
    - Improved inference speed of the **Whisper** series by over 10%.
    - Supported the following large language models (LLMs) with **ONNX Runtime 100% GPU operator deployment**:
        - Qwen3-4B/8B
        - InternLM3-8B
        - Phi-4-mini-Instruct
        - Gemma3-4B/12B-it
    - Expanded hardware support:
        - **Intel OpenVINO**
        - **NVIDIA CUDA GPU**
        - **Windows DirectML GPU** (supports integrated and discrete GPUs)  
---

## âœ¨ Features  
This project is built on ONNX Runtime framework.
- Deoiser Support:
  - [DFSMN](https://modelscope.cn/models/iic/speech_dfsmn_ans_psm_48k_causal)
  - [GTCRN](https://github.com/Xiaobin-Rong/gtcrn)
  - [ZipEnhancer](https://modelscope.cn/models/iic/speech_zipenhancer_ans_multiloss_16k_base)
  - [Mel-Band-Roformer](https://github.com/KimberleyJensen/Mel-Band-Roformer-Vocal-Model)
  - [MossFormerGAN_SE_16K](https://www.modelscope.cn/models/alibabasglab/MossFormerGAN_SE_16K)

- VAD Support:
  - [FSMN](https://modelscope.cn/models/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch)
  - [Faster_Whisper - Silero](https://github.com/SYSTRAN/faster-whisper/blob/master/faster_whisper/vad.py)
  - [Official - Silero](https://github.com/snakers4/silero-vad)
  - [HumAware](https://huggingface.co/CuriousMonkey7/HumAware-VAD)
  - [NVIDIA-NeMo-VAD-v2.0](https://huggingface.co/nvidia/Frame_VAD_Multilingual_MarbleNet_v2.0)
  - [TEN-VAD](https://github.com/TEN-framework/ten-vad)
  - [Pyannote-Segmentation-3.0](https://huggingface.co/pyannote/segmentation-3.0)
    - You need to accept Pyannote's terms of use and download the Pyannote `pytorch_model.bin` file. Next, place it in the `VAD/pyannote_segmentation` folder.

- ASR Support:
  - [SenseVoice-Small](https://modelscope.cn/models/iic/SenseVoiceSmall)
  - [Paraformer-Small-Chinese](https://modelscope.cn/models/iic/speech_paraformer_asr_nat-zh-cn-16k-common-vocab8358-tensorflow1)
  - [Paraformer-Large-Chinese](https://modelscope.cn/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch)
  - [Paraformer-Large-English](https://modelscope.cn/models/iic/speech_paraformer_asr-en-16k-vocab4199-pytorch)
  - [Whisper-Large-V3](https://huggingface.co/openai/whisper-large-v3)
  - [Whisper-Large-V3-Turbo](https://huggingface.co/openai/whisper-large-v3-turbo)
  - [Whisper-Large-V3-Turbo-Japanese](https://huggingface.co/hhim8826/whisper-large-v3-turbo-ja)
  - [Whisper-Large-V3-Anime-A](https://huggingface.co/efwkjn/whisper-ja-anime-v0.1)
  - [Whisper-Large-V3-Anime-B](https://huggingface.co/litagin/anime-whisper)
  - [Whisper-Large-v3.5-Distil](https://huggingface.co/distil-whisper/distil-large-v3.5)
  - [CrisperWhisper](https://github.com/nyrahealth/CrisperWhisper)
  - [FireRedASR-AED-L](https://github.com/FireRedTeam/FireRedASR)
  - [Dolphin-Small](https://github.com/DataoceanAI/Dolphin)

- LLM Supports: 
  - Qwen-3: [4B](https://modelscope.cn/models/Qwen/Qwen3-4B), [8B](https://modelscope.cn/models/Qwen/Qwen3-8B)
  - InternLM-3: [8B](https://huggingface.co/internlm/internlm3-8b-instruct)
  - Gemma-3-it: [4B](https://huggingface.co/google/gemma-3-4b-it), [12B](https://huggingface.co/google/gemma-3-12b-it)  
  - Phi-4-Instruct: [mini](https://huggingface.co/microsoft/Phi-4-mini-instruct)
---

## ğŸ“‹ Setup Instructions

### âœ… Step 1: Install Dependencies
- Run the following command in your terminal to install the latest required Python packages:
- For Apple Silicon M-series chips, avoid installing `onnxruntime-openvino`, as it will cause errors.
```bash
conda install ffmpeg

pip install -r requirements.txt
```

### ğŸ“¥ Step 2: Download Necessary Models
- Download the required models from HuggingFace: [Transcribe_and_Translate_Subtitles](https://huggingface.co/H5N1AIDS/Transcribe_and_Translate_Subtitles).


### ğŸ–¥ï¸ Step 3: Download and Place `run.py`
- Download the `run.py` script from this repository.
- Place it in the `Transcribe_and_Translate_Subtitles` folder.

### ğŸ“ Step 4: Place Target Videos in the Media Folder
- Place the videos you want to transcribe and translate in the following directory. The application will process the videos one by one.:
```
Transcribe_and_Translate_Subtitles/Media
```

### ğŸš€ Step 5: Run the Application
- Open your preferred terminal (PyCharm, CMD, PowerShell, etc.).
- Execute the following command to start the application:
```bash
python run.py
```
- Once the application starts, you will see a webpage open in your browser.
   ![screenshot](https://github.com/DakeQQ/Transcribe-and-Translate-Subtitles/blob/main/screen/Screenshot%20from%202025-05-08%2013-01-17.png)

### ğŸ› ï¸ Step 6: Fix Error (if encountered)
- On the first run, you might encounter a **Silero-VAD error**. Simply restart the application, and it should be resolved.
- On the first run, you might encounter a **libc++1.so error**. Run the following commands in the terminal, and they should resolve the issue.
```bash
sudo apt update
sudo apt install libc++1
```

### ğŸ’» Step 7: Device Support
- This project currently supports:
  - **Intel-OpenVINO-CPU-GPU-NPU**
  - **Windows-AMD-GPU**
  - **NVIDIA-GPU**
  - **Apple-CPU**
  - **AMD-CPU**

---

## ğŸ‰ Enjoy the Application!
```
Transcribe_and_Translate_Subtitles/Results/Subtitles
```
---

## ğŸ“Œ To-Do List
- [ ] [LLM-MiniCPM4](https://github.com/OpenBMB/MiniCPM)
- [ ] [Denoiser-MossFormer2-48K](https://www.modelscope.cn/models/alibabasglab/MossFormer2_SE_48K)
- [ ] [Upscale the Resolution of Audio-MossFormer2-48K](https://www.modelscope.cn/models/alibabasglab/MossFormer2_SR_48K)
- [ ] [Upscale the Resolution of Video](https://github.com/sczhou/Upscale-A-Video)
- [ ] AMD-ROCm Support
- [ ] Real-Time Translate & Trascribe Video Player

---
### æ€§èƒ½ Performance  
| OS           | Backend           | Denoiser          | VAD                  | ASR                  | LLM | Real-Time Factor<br>test_video.mp4<br>7602 seconds |
|:------------:|:-----------------:|:-----------------:|:--------------------:|:--------------------:|:----------------:|:----------------:|
| Ubuntu-24.04 | CPU <br> i3-12300 | -                 | Silero               | SenseVoiceSmall      |        -      |        0.08      |
| Ubuntu-24.04 | CPU <br> i3-12300 | GTCRN             | Silero               | SenseVoiceSmall      | Qwen2.5-7B-Instruct | 0.50       |
| Ubuntu-24.04 | CPU <br> i3-12300 | GTCRN             | FSMN                 | SenseVoiceSmall      |        -      |       0.054      |
| Ubuntu-24.04 | CPU <br> i3-12300 | ZipEnhancer       | FSMN                 | SenseVoiceSmall      |        -      |        0.39      |
| Ubuntu-24.04 | CPU <br> i3-12300 | GTCRN             | Silero               | Whisper-Large-V3     |        -      |        0.20      |
| Ubuntu-24.04 | CPU <br> i3-12300 | GTCRN             | FSMN                 | Whisper-Large-V3-Turbo |      -      |        0.148     |

---
# è½¬å½•å’Œç¿»è¯‘å­—å¹•

## ğŸš¨ é‡è¦æç¤º  
- **æ‰€æœ‰ä»»åŠ¡å‡åœ¨æœ¬åœ°è¿è¡Œï¼Œæ— éœ€è¿æ¥äº’è”ç½‘ï¼Œç¡®ä¿æœ€å¤§ç¨‹åº¦çš„éšç§ä¿æŠ¤ã€‚**

---
## æœ€è¿‘æ›´æ–°ä¸åŠŸèƒ½
- 2025/7/5
    - æ–°å¢ é™å™ª MossFormerGAN_SE_16K
- 2025/6/11
    - æ–°å¢ HumAware-VAD, NVIDIA-NeMo-VAD, TEN-VADã€‚
- 2025/6/3
    - æ–°å¢ Dolphin ASR æ¨¡å‹ä»¥æ”¯æŒäºšæ´²è¯­è¨€ã€‚
- 2025/5/13
    - æ–°å¢ Float16/32 ASR æ¨¡å‹ï¼Œæ”¯æŒ CUDA/DirectML GPU ä½¿ç”¨ã€‚è¿™äº›æ¨¡å‹å¯å®ç° >99% çš„ GPU ç®—å­éƒ¨ç½²ç‡ã€‚
- 2025/5/9
    - æ–°å¢ **ä¸ä½¿ç”¨** VADï¼ˆè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼‰çš„é€‰é¡¹ï¼Œæä¾›æ›´å¤šçµæ´»æ€§ã€‚
    - æ–°å¢é™å™ªæ¨¡å‹ï¼š**MelBandRoformer**ã€‚
    - æ–°å¢ä¸‰æ¬¾æ—¥è¯­åŠ¨æ¼«å¾®è°ƒWhisperæ¨¡å‹ã€‚
    - æ–°å¢ASRæ¨¡å‹ï¼š**CrisperWhisper**ã€‚
    - æ–°å¢è‹±è¯­å¾®è°ƒASRæ¨¡å‹ï¼š**Whisper-Large-v3.5-Distil**ã€‚
    - æ–°å¢æ”¯æŒä¸­æ–‡ï¼ˆåŒ…æ‹¬éƒ¨åˆ†æ–¹è¨€ï¼‰çš„ASRæ¨¡å‹ï¼š**FireRedASR-AED-L**ã€‚
    - ç§»é™¤IPEX-LLMæ¡†æ¶ï¼Œæå‡æ•´ä½“æ€§èƒ½ã€‚
    - å–æ¶ˆLLMé‡åŒ–é€‰é¡¹ï¼Œç»Ÿä¸€é‡‡ç”¨**Q4F32**æ ¼å¼ã€‚
    - æ”¹è¿›äº†**FSMN-VAD**çš„å‡†ç¡®ç‡ã€‚
    - æ”¹è¿›äº†**Paraformer**çš„è¯†åˆ«å‡†ç¡®ç‡ã€‚
    - æ”¹è¿›äº†**SenseVoice**çš„è¯†åˆ«å‡†ç¡®ç‡ã€‚
    - æ”¹è¿›äº†**Whisper**ç³»åˆ—çš„æ¨ç†é€Ÿåº¦10%+ã€‚
    - æ”¯æŒä»¥ä¸‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå®ç°**ONNX Runtime 100% GPUç®—å­éƒ¨ç½²**ï¼š
        - Qwen3-4B/8B  
        - InternLM3-8B  
        - Phi-4-mini-Instruct  
        - Gemma3-4B/12B-it
    - æ‰©å±•ç¡¬ä»¶æ”¯æŒï¼š  
        - **Intel OpenVINO**  
        - **NVIDIA CUDA GPU**  
        - **Windows DirectML GPU**ï¼ˆæ”¯æŒé›†æˆæ˜¾å¡å’Œç‹¬ç«‹æ˜¾å¡ï¼‰
---

## âœ¨ åŠŸèƒ½
è¿™ä¸ªé¡¹ç›®åŸºäº ONNX Runtime æ¡†æ¶ã€‚
- **å»å™ªå™¨ (Denoiser) æ”¯æŒ**ï¼š
  - [DFSMN](https://modelscope.cn/models/iic/speech_dfsmn_ans_psm_48k_causal)
  - [GTCRN](https://github.com/Xiaobin-Rong/gtcrn)
  - [ZipEnhancer](https://modelscope.cn/models/iic/speech_zipenhancer_ans_multiloss_16k_base)
  - [Mel-Band-Roformer](https://github.com/KimberleyJensen/Mel-Band-Roformer-Vocal-Model)
  - [MossFormerGAN_SE_16K](https://www.modelscope.cn/models/alibabasglab/MossFormerGAN_SE_16K)
 
- **è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆVADï¼‰æ”¯æŒ**ï¼š
  - [FSMN](https://modelscope.cn/models/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch)
  - [Faster_Whisper - Silero](https://github.com/SYSTRAN/faster-whisper/blob/master/faster_whisper/vad.py)
  - [å®˜æ–¹ - Silero](https://github.com/snakers4/silero-vad)
  - [HumAware](https://huggingface.co/CuriousMonkey7/HumAware-VAD)
  - [NVIDIA-NeMo-VAD-v2.0](https://huggingface.co/nvidia/Frame_VAD_Multilingual_MarbleNet_v2.0)
  - [TEN-VAD](https://github.com/TEN-framework/ten-vad)
  - [Pyannote-Segmentation-3.0](https://huggingface.co/pyannote/segmentation-3.0)
    - éœ€è¦æ¥å—Pyannoteçš„ä½¿ç”¨æ¡æ¬¾ï¼Œä¸¦è‡ªè¡Œä¸‹è½½ Pyannote `pytorch_model.bin` æ–‡ä»¶ï¼Œå¹¶å°†å…¶æ”¾ç½®åœ¨ `VAD/pyannote_segmentation` æ–‡ä»¶å¤¹ä¸­ã€‚

- **è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ”¯æŒ**ï¼š
  - [SenseVoice-Small](https://modelscope.cn/models/iic/SenseVoiceSmall)
  - [Paraformer-Small-Chinese](https://modelscope.cn/models/iic/speech_paraformer_asr_nat-zh-cn-16k-common-vocab8358-tensorflow1)
  - [Paraformer-Large-Chinese](https://modelscope.cn/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch)
  - [Paraformer-Large-English](https://modelscope.cn/models/iic/speech_paraformer_asr-en-16k-vocab4199-pytorch)
  - [Whisper-Large-V3](https://huggingface.co/openai/whisper-large-v3)
  - [Whisper-Large-V3-Turbo](https://huggingface.co/openai/whisper-large-v3-turbo)
  - [Whisper-Large-V3-Turbo-Japanese](https://huggingface.co/hhim8826/whisper-large-v3-turbo-ja)
  - [Whisper-Large-V3-Anime-A](https://huggingface.co/efwkjn/whisper-ja-anime-v0.1)
  - [Whisper-Large-V3-Anime-B](https://huggingface.co/litagin/anime-whisper)
  - [Whisper-Large-v3.5-Distil](https://huggingface.co/distil-whisper/distil-large-v3.5)
  - [CrisperWhisper](https://github.com/nyrahealth/CrisperWhisper)
  - [FireRedASR-AED-L](https://github.com/FireRedTeam/FireRedASR)
  - [Dolphin-Small](https://github.com/DataoceanAI/Dolphin)


- **å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ”¯æŒ**ï¼š  
  - Qwen-3: [4B](https://modelscope.cn/models/Qwen/Qwen3-4B), [8B](https://modelscope.cn/models/Qwen/Qwen3-8B)
  - InternLM-3: [8B](https://huggingface.co/internlm/internlm3-8b-instruct)
  - Gemma-3-it: [4B](https://huggingface.co/google/gemma-3-4b-it), [12B](https://huggingface.co/google/gemma-3-12b-it)  
  - Phi-4-Instruct: [mini](https://huggingface.co/microsoft/Phi-4-mini-instruct)

---

## ğŸ“‹ è®¾ç½®æŒ‡å—

### âœ… ç¬¬ä¸€æ­¥ï¼šå®‰è£…ä¾èµ–é¡¹  
- åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…æ‰€éœ€çš„æœ€æ–° Python åŒ…ï¼š
- å¯¹äºè‹¹æœ M ç³»åˆ—èŠ¯ç‰‡ï¼Œè¯·ä¸è¦å®‰è£… `onnxruntime-openvino`ï¼Œå¦åˆ™ä¼šå¯¼è‡´é”™è¯¯ã€‚
```bash
conda install ffmpeg

pip install -r requirements.txt
```

### ğŸ“¥ ç¬¬äºŒæ­¥ï¼šä¸‹è½½å¿…è¦çš„æ¨¡å‹  
- ä» HuggingFace ä¸‹è½½æ‰€éœ€æ¨¡å‹ï¼š[Transcribe_and_Translate_Subtitles](https://huggingface.co/H5N1AIDS/Transcribe_and_Translate_Subtitles)


### ğŸ–¥ï¸ ç¬¬ä¸‰æ­¥ï¼šä¸‹è½½å¹¶æ”¾ç½® `run.py`  
- ä»æ­¤é¡¹ç›®çš„ä»“åº“ä¸‹è½½ `run.py` è„šæœ¬ã€‚  
- å°† `run.py` æ”¾ç½®åœ¨ `Transcribe_and_Translate_Subtitles` æ–‡ä»¶å¤¹ä¸­ã€‚

### ğŸ“ ç¬¬å››æ­¥ï¼šå°†ç›®æ ‡è§†é¢‘æ”¾å…¥ Media æ–‡ä»¶å¤¹  
- å°†ä½ æƒ³è¦è½¬å½•å’Œç¿»è¯‘çš„è§†é¢‘æ”¾ç½®åœ¨ä»¥ä¸‹ç›®å½•ï¼Œåº”ç”¨ç¨‹åºå°†é€ä¸ªå¤„ç†è¿™äº›è§†é¢‘ï¼š  
```
Transcribe_and_Translate_Subtitles/Media
```

### ğŸš€ ç¬¬äº”æ­¥ï¼šè¿è¡Œåº”ç”¨ç¨‹åº  
- æ‰“å¼€ä½ å–œæ¬¢çš„ç»ˆç«¯å·¥å…·ï¼ˆPyCharmã€CMDã€PowerShell ç­‰ï¼‰ã€‚  
- è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å¯åŠ¨åº”ç”¨ç¨‹åºï¼š  
```bash
python run.py
```
- åº”ç”¨ç¨‹åºå¯åŠ¨åï¼Œä½ çš„æµè§ˆå™¨å°†è‡ªåŠ¨æ‰“å¼€ä¸€ä¸ªç½‘é¡µã€‚  
   ![screenshot](https://github.com/DakeQQ/Transcribe-and-Translate-Subtitles/blob/main/screen/Screenshot%20from%202025-05-08%2013-01-17.png)

### ğŸ› ï¸ ç¬¬å…­æ­¥ï¼šä¿®å¤é”™è¯¯ï¼ˆå¦‚æœ‰ï¼‰  
- é¦–æ¬¡è¿è¡Œæ—¶ï¼Œä½ å¯èƒ½ä¼šé‡åˆ° **Silero-VAD é”™è¯¯**ã€‚åªéœ€é‡å¯åº”ç”¨ç¨‹åºå³å¯è§£å†³è¯¥é—®é¢˜ã€‚
- é¦–æ¬¡è¿è¡Œæ—¶ï¼Œä½ å¯èƒ½ä¼šé‡åˆ° **libc++1.so é”™è¯¯**ã€‚åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œåº”è¯¥å¯ä»¥è§£å†³é—®é¢˜ã€‚
```bash
sudo apt update
sudo apt install libc++1
```

### ğŸ’» ç¬¬ä¸ƒæ­¥ï¼šæ”¯æŒè®¾å¤‡  
- æ­¤é¡¹ç›®ç›®å‰æ”¯æŒ:
  - **Intel-OpenVINO-CPU-GPU-NPU**
  - **Windows-AMD-GPU**
  - **NVIDIA-GPU**
  - **Apple-CPU**
  - **AMD-CPU**

## ğŸ‰ å°½æƒ…äº«å—åº”ç”¨ç¨‹åºå§ï¼
```
Transcribe_and_Translate_Subtitles/Results/Subtitles
```
---

## ğŸ“Œ å¾…åŠäº‹é¡¹  
- [ ] [LLM-MiniCPM4](https://github.com/OpenBMB/MiniCPM)
- [ ] [å»å™ª-MossFormer2-48K](https://www.modelscope.cn/models/alibabasglab/MossFormer2_SE_48K)
- [ ] [æé«˜éŸ³é¢‘è´¨é‡-MossFormer2-48K](https://www.modelscope.cn/models/alibabasglab/MossFormer2_SR_48K)
- [ ] [æé«˜è§†é¢‘åˆ†è¾¨ç‡](https://github.com/sczhou/Upscale-A-Video)
- [ ] æ”¯æŒ AMD-ROCm
- [ ] å®ç°å®æ—¶è§†é¢‘è½¬å½•å’Œç¿»è¯‘æ’­æ”¾å™¨
---
